{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an online perceptron for text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online learning is a dynamic approach to machine learning, where at each step, we get a new observation for which a prediction is made. After the prediction, we get to observe the actual label of the data and use it to update the estimation function. \n",
    "\n",
    "Here, we attempt to build an online perceptron, coded manually, which predicts restaurant recommendation based on customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import heapq\n",
    "import nltk\n",
    "from scipy.sparse import hstack,csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame.from_csv(\"reviews_te.csv\")\n",
    "#df_test=df_test[:1000]\n",
    "y_test=df_test.index.values\n",
    "y_test[y_test==0]=-1\n",
    "df_train=pd.DataFrame.from_csv(\"reviews_tr.csv\")\n",
    "#df_train=df_train[:10000]\n",
    "y_train=df_train.index.values\n",
    "y_train[y_train==0]=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the perceptron using 4 different representations of the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Term frequency\n",
    "\n",
    "Here, each word in a document is represented by a feature that counts the number of times it appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_constant(matrix):\n",
    "    return(hstack((matrix,np.ones(matrix.shape[0]).astype(np.int64)[:,None]),format=\"csr\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "sparse_train =add_constant(vectorizer.fit_transform(df_train.text.values))\n",
    "sparse_test = add_constant(vectorizer.transform(df_test.text.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perceptron works by actualizing the weights at each step. If $w.x > 0$, the perceptron returns a correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def online_perceptron(X,y,n):\n",
    "    w=np.zeros(X.shape[1])\n",
    "    w_sum=np.zeros(X.shape[1])\n",
    "    index = np.arange(np.shape(sparse_train)[0])\n",
    "    for i in range(n):\n",
    "        np.random.shuffle(index)\n",
    "        X_shuffle=X[index, :]\n",
    "        y_shuffle=y[index]\n",
    "        count=0\n",
    "        k=0\n",
    "        while k<len(y):\n",
    "            while k<len(y) and y_shuffle[k]*X_shuffle[k].dot(w)[0]>0:#use sparse\n",
    "                if i==n-1:\n",
    "                    count+=1\n",
    "                k+=1\n",
    "            if i==n-1:\n",
    "                w_sum=w_sum+count*np.array(w)\n",
    "            if k<len(y):\n",
    "                w=np.array(w+y_shuffle[k]*X_shuffle[k])[0]#use sparse\n",
    "            k+=1\n",
    "            count=1\n",
    "    return(w_sum/(len(y)+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the perceptron on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_final=online_perceptron(sparse_train, y_train, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words with lowest (most negative) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'flavorless',\n",
       " u'hopes',\n",
       " u'inedible',\n",
       " u'lacked',\n",
       " u'mediocre',\n",
       " u'meh',\n",
       " u'poisoning',\n",
       " u'underwhelmed',\n",
       " u'underwhelming',\n",
       " u'worst']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([vectorizer.get_feature_names()[i] for i in w_final.argsort()[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 words with highest (most positive) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disappoint',\n",
       " u'exceeded',\n",
       " u'gem',\n",
       " u'heavenly',\n",
       " u'incredible',\n",
       " u'perfection',\n",
       " u'skeptical',\n",
       " u'worried',\n",
       " u'yerm',\n",
       " u'yurm']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([vectorizer.get_feature_names()[i] for i in (-w_final).argsort()[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using a loss function: the risk, which indicates the frequency of false predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def risk_train(X,y,w):\n",
    "    y_pred_train=X.dot(w)\n",
    "    y_pred_train[y_pred_train<=0]=-1\n",
    "    y_pred_train[y_pred_train>0]=1\n",
    "    return(np.mean(abs(y_pred_train-y))/2)\n",
    "\n",
    "def risk_test(X,y,w):\n",
    "    y_pred_test=X.dot(w)\n",
    "    y_pred_test[y_pred_test<=0]=-1\n",
    "    y_pred_test[y_pred_test>0]=1\n",
    "    return(np.mean(abs(y_pred_test-y))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.101561"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_train(sparse_train, y_train, w_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10525986967468652"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_test(sparse_test, y_test, w_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Term frequency-inverse document frequency (tf-idf).\n",
    "\n",
    "Tf-idf is an improvement of the term frequency representation. It basically corrects term frequency by multiplying it by the log of the inverse of the frequency of the word through all the training documents. This is beneficial because it allows to increase the value for words that appear a lot in one document as opposed to others, and thus highlights each documents' specificities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_vectorizer=TfidfVectorizer()\n",
    "sparse_train= add_constant(idf_vectorizer.fit_transform(df_train.text.values))\n",
    "sparse_test= add_constant(idf_vectorizer.transform(df_test.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_final_idf=online_perceptron(sparse_train, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.095028000000000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_train(sparse_train, y_train, w_final_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10616265048950088"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_test(sparse_test, y_test, w_final_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, it did not improve the test loss, and slightly improved the train loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram representation.\n",
    "\n",
    "The downside of the two previous approaches is that they do not consider the order of the words, but only their frequency. In \"I do not like apples\" and \"I like apples\", both sentences have the words \"like\" and \"apples, but they have different meanings because of the expression \"not like\" in the former. One way to account for this is by using 2gram representations, which is the same as before, but for any combination of 2 consecutive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "sparse_train= add_constant(bigram_vectorizer.fit_transform(df_train.text.values))\n",
    "sparse_test= add_constant(bigram_vectorizer.transform(df_test.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_final_bigram=online_perceptron(sparse_train, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068169999999999994"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_train(sparse_train, y_train, w_final_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089034805480410595"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_test(sparse_test, y_test, w_final_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this significantly improve our score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming\n",
    "We can also use stemming to allow \"apples\" and \"apple\" to have the same frequency countings by reducing each word to an arbitrary root. Then, we will remove stopwords (\"the\", \"a\", \"is\", ...) and use the 2gram representation on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "theStemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df,stopWords=stopWords,theStemmer=theStemmer):\n",
    "    df_copy=df\n",
    "    for k in range(len(df_copy)):\n",
    "        tokens = [word for word in df_copy.text.values[k].split(\" \") if word not in stopWords]\n",
    "        tokens = [theStemmer.stem(word) for word in tokens] #stem words uing porter stemming algorithm\n",
    "        df_copy.text.values[k]= \" \".join(tokens)\n",
    "    return(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train=clean_data(df_train)\n",
    "clean_test=clean_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer_clean = CountVectorizer(ngram_range=(1, 2))\n",
    "sparse_train= add_constant(bigram_vectorizer_clean.fit_transform(clean_train.text.values))\n",
    "sparse_test= add_constant(bigram_vectorizer_clean.transform(clean_test.text.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_final_bigram_clean=online_perceptron(sparse_train, y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk_train(sparse_train, y_train, w_final_bigram_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "risk_test(sparse_test, y_test, w_final_bigram_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
